{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66df462d-f139-4a57-8bd9-041f528d800d",
   "metadata": {},
   "source": [
    "# Silver layer\n",
    "# Passaggio 0: Verifica e caricamento dei dati\n",
    "Questo primo blocco carica i dati e stampa a schermo la data più vecchia e quella più recente che trova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c59ef-ac1a-4caf-bcbd-81344f2cce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio Fase 1: Caricamento e Preparazione...\n",
      "Tabelle Bronze caricate.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deltalake import DeltaTable, write_deltalake\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "print(\"Inizio Fase 1: Caricamento e Preparazione...\")\n",
    "\n",
    "\n",
    "# ================== CONFIGURAZIONE ==================\n",
    "storage_options = { \n",
    "    \"AWS_ACCESS_KEY_ID\": os.getenv(\"AWS_ACCESS_KEY_ID\"),                \n",
    "    \"AWS_SECRET_ACCESS_KEY\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"), \n",
    "    \"AWS_ENDPOINT_URL\": os.getenv(\"AWS_ENDPOINT_URL\"), \n",
    "    \"AWS_ALLOW_HTTP\": \"true\" \n",
    "}\n",
    "\n",
    "try:\n",
    "    df_misure_raw = DeltaTable(\"s3a://external/qualita_aria_bronze\", storage_options=storage_options).to_pandas()\n",
    "    df_stazioni_anagrafica = DeltaTable(\"s3a://external/anagrafica_stazioni_bronze\", storage_options=storage_options).to_pandas()\n",
    "    df_parametri_anagrafica = DeltaTable(\"s3a://external/anagrafica_parametri_bronze\", storage_options=storage_options).to_pandas()\n",
    "    print(\"Tabelle Bronze caricate.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRORE CRITICO: Impossibile caricare le tabelle. Dettagli: {e}\")\n",
    "\n",
    "df_misure_raw.rename(columns={'COD_STAZ': 'id_stazione', 'ID_PARAM': 'id_parametro'}, inplace=True)\n",
    "df_parametri_anagrafica.rename(columns={'IdParametro': 'id_parametro'}, inplace=True)\n",
    "df_stazioni_anagrafica.rename(columns={'Cod_staz': 'id_stazione'}, inplace=True)\n",
    "df_lavoro = pd.merge(df_misure_raw, df_parametri_anagrafica[['id_parametro', 'PARAMETRO']], on='id_parametro', how='left')\n",
    "\n",
    "df_lavoro['DATA_INIZIO'] = pd.to_datetime(df_lavoro['DATA_INIZIO'])\n",
    "df_lavoro['VALORE'] = pd.to_numeric(df_lavoro['VALORE'], errors='coerce')\n",
    "df_lavoro.dropna(subset=['VALORE', 'PARAMETRO'], inplace=True)\n",
    "\n",
    "df_stazioni_anagrafica['id_stazione'] = pd.to_numeric(df_stazioni_anagrafica['id_stazione'], errors='coerce')\n",
    "\n",
    "\n",
    "print(\"df_misure_raw------------------------------------------------------\")\n",
    "df_misure_raw.info()\n",
    "print(\"df_parametri_anagrafica------------------------------------------------------\")\n",
    "df_parametri_anagrafica.info()\n",
    "print(\"df_stazioni_anagrafica------------------------------------------------------\")\n",
    "df_stazioni_anagrafica.info()\n",
    "print(\"df_lavoro------------------------------------------------------\")\n",
    "df_lavoro.info()\n",
    "\n",
    "print(\"DataFrame di lavoro pronto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f5014c-6f2d-453d-9e97-5a9f19a29237",
   "metadata": {},
   "source": [
    "# Passaggio 1: Preparazione del DataFrame di Lavoro\n",
    "Ora che abbiamo verificato i dati, li prepariamo per la trasformazione. Questo include la pulizia dei nomi, l'unione delle informazioni e la conversione dei tipi di dato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c94825-e70b-4110-8074-571c9a16eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inizio Fase 1: Preparazione DataFrame di Lavoro...\")\n",
    "\n",
    "# Standardizziamo i nomi delle colonne chiave per unire i dati in modo affidabile\n",
    "df_misure_raw.rename(columns={'COD_STAZ': 'id_stazione', 'ID_PARAM': 'id_parametro'}, inplace=True)\n",
    "df_parametri_anagrafica.rename(columns={'IdParametro': 'id_parametro'}, inplace=True)\n",
    "df_stazioni_anagrafica.rename(columns={'Cod_staz': 'id_stazione'}, inplace=True)\n",
    "\n",
    "# Uniamo le misurazioni con il nome del parametro per avere un contesto\n",
    "df_lavoro = pd.merge(df_misure_raw, df_parametri_anagrafica[['id_parametro', 'PARAMETRO']], on='id_parametro', how='left')\n",
    "\n",
    "# Convertiamo il valore in numerico, gestendo eventuali errori\n",
    "df_lavoro['VALORE'] = pd.to_numeric(df_lavoro['VALORE'], errors='coerce')\n",
    "# Rimuoviamo righe dove il valore non è valido o il nome del parametro è mancante\n",
    "rows_before = len(df_lavoro)\n",
    "df_lavoro.dropna(subset=['VALORE', 'PARAMETRO'], inplace=True)\n",
    "rows_after = len(df_lavoro)\n",
    "print(f\"Rimossi {rows_before - rows_after} record con date o valori non validi.\")\n",
    "df_lavoro.info()\n",
    "\n",
    "print(\"DataFrame di lavoro pronto per la Feature Engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6bf3a-257b-42a7-8404-2c9e39ee18b5",
   "metadata": {},
   "source": [
    "# Passaggio 2: Feature Engineering (Costruzione del Profilo Stazione)\n",
    "Questa è la fase centrale, dove creiamo le 6 feature che descrivono ogni stazione. Ogni blocco di codice calcola una o più feature specifiche.\n",
    "è stato dei\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38d5a9-a22d-48bb-a475-710fb603858f",
   "metadata": {},
   "source": [
    "### Feature 1 & 2: Media e Variabilità Generale\n",
    "\n",
    "**Obiettivo:** Calcolare le due statistiche più fondamentali per ogni inquinante misurato da una stazione.\n",
    "- **Media (`mean`):** Rappresenta il \"livello di base\" di inquinamento. Ci dice se una stazione si trova, in media, in una zona più o meno inquinata.\n",
    "- **Deviazione Standard (`std`):** Rappresenta la \"variabilità\" o \"nervosismo\" della stazione. Ci dice se i livelli di inquinamento sono stabili o se tendono ad avere forti picchi e crolli.\n",
    "\n",
    "**Risultato:** Il DataFrame `df_stats` conterrà una riga per ogni stazione e una colonna per la media e la deviazione standard di *ciascun* inquinante (es. `mean_PM10`, `std_PM10`, `mean_NO2_Biossido_di_azoto`, ecc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282126c2-96bb-43ca-96a8-7574baf306a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1, 2. Calcolo di Media e Deviazione Standard...\")\n",
    "df_stats = df_lavoro.pivot_table(index='id_stazione', columns='PARAMETRO', values='VALORE', aggfunc=['mean', 'std'])\n",
    "df_stats.columns = [f'{stat}_{param.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")}' for stat, param in df_stats.columns]\n",
    "df_stats.info()\n",
    "#print(df_stats)\n",
    "print(\"1, 2. Calcolo di Media e Deviazione Standard completato\")\n",
    "# il risultato: ci saranno una colonna per media e dev std per ogni inquinante registrato e una entry per ogni stazione. Il codice stazione non è più una colonna ma l'indice di accesso ad una riga\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439711f-63ca-4031-8752-54ea55ec17eb",
   "metadata": {},
   "source": [
    "### Feature 3: Ciclo Settimanale (Impronta Umana)\n",
    "\n",
    "**Obiettivo:** Quantificare l'impatto delle attività umane (come traffico e lavoro) su ogni inquinante. Lo facciamo calcolando il rapporto tra la concentrazione media nei giorni feriali (Lunedì-Venerdì) e quella nei giorni festivi (Sabato-Domenica).\n",
    "\n",
    "- Un valore **> 1** suggerisce una forte influenza delle attività settimanali (tipico delle stazioni \"da traffico\").\n",
    "- Un valore **~ 1** suggerisce un'influenza scarsa o nulla (tipico delle stazioni \"di fondo\" o rurali).\n",
    "\n",
    "**Risultato:** Il DataFrame `df_rapporto_settimanale` conterrà una riga per ogni stazione e una colonna per questo rapporto, calcolato per ogni inquinante (es. `rapporto_feriale_festivo_NO2_Biossido_di_azoto`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a53ed0d-48de-4cc1-aef4-883fdda6b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3. Calcolo del Ciclo Settimanale per tutti gli inquinanti...\")\n",
    "df_lavoro['feriale'] = df_lavoro['DATA_INIZIO'].dt.dayofweek < 5\n",
    "df_medie_periodo = df_lavoro.pivot_table(index='id_stazione', columns=['PARAMETRO', 'feriale'], values='VALORE', aggfunc='mean')\n",
    "df_rapporto_settimanale = pd.DataFrame(index=df_medie_periodo.index)\n",
    "for param in df_lavoro['PARAMETRO'].unique():\n",
    "    if (param, True) in df_medie_periodo.columns and (param, False) in df_medie_periodo.columns:\n",
    "        nome_colonna = f'rapporto_feriale_festivo_{param.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")}'\n",
    "        df_rapporto_settimanale[nome_colonna] = df_medie_periodo[(param, True)] / df_medie_periodo[(param, False)]\n",
    "\n",
    "df_medie_periodo.info()\n",
    "print(\"------------------------------------------------------\")\n",
    "df_rapporto_settimanale.info()\n",
    "\n",
    "\n",
    "print(\"3. Calcolo del Ciclo Settimanale per NO2 completato\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c3ffd-b203-4064-9cef-741018613ff5",
   "metadata": {},
   "source": [
    "### Feature 4: Ciclo Stagionale (Impronta Climatica)\n",
    "\n",
    "**Obiettivo:** Capire come il profilo di ogni inquinante cambia con le stagioni. Calcoliamo la concentrazione media per ciascuna delle quattro stagioni (inverno, primavera, estate, autunno). Questo ci permette di identificare pattern legati al clima, come l'accumulo di PM10 in inverno a causa delle condizioni meteorologiche o i picchi di Ozono in estate.\n",
    "\n",
    "**Risultato:** Il DataFrame `df_medie_stagionali` conterrà una riga per ogni stazione e quattro colonne per ogni inquinante, una per la media di ogni stagione (es. `media_inverno_PM10`, `media_primavera_PM10`, ecc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307dbba-f03c-4a7e-abf0-2e1a816e9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4. Calcolo del Ciclo Stagionale a 4 stagioni per tutti gli inquinanti...\")\n",
    "\n",
    "# 1. Definiamo i mesi per ogni stagione come da te specificato\n",
    "stagioni_mapping = {\n",
    "    1: 'inverno', 2: 'inverno', 3: 'primavera',\n",
    "    4: 'primavera', 5: 'primavera', 6: 'estate',\n",
    "    7: 'estate', 8: 'estate', 9: 'autunno',\n",
    "    10: 'autunno', 11: 'autunno', 12: 'inverno'\n",
    "}\n",
    "\n",
    "# 2. Creiamo una colonna 'stagione' nel DataFrame di lavoro\n",
    "df_lavoro['mese'] = df_lavoro['DATA_INIZIO'].dt.month\n",
    "df_lavoro['stagione'] = df_lavoro['mese'].map(stagioni_mapping)\n",
    "\n",
    "# 3. Calcoliamo la media per ogni inquinante in ciascuna delle 4 stagioni.\n",
    "#    pivot_table è perfetto per questo: creerà una colonna per ogni combinazione\n",
    "#    di inquinante e stagione (es. PM10-inverno, PM10-primavera, etc.).\n",
    "df_medie_stagionali = df_lavoro.pivot_table(\n",
    "    index='id_stazione', \n",
    "    columns=['PARAMETRO', 'stagione'], \n",
    "    values='VALORE', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# 4. Rinominiamo le colonne per renderle pulite e utilizzabili,\n",
    "#    ad esempio ('PM10', 'inverno') diventa 'media_inverno_PM10'.\n",
    "df_medie_stagionali.columns = [\n",
    "    f'media_{stagione}_{param.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")}' \n",
    "    for param, stagione in df_medie_stagionali.columns\n",
    "]\n",
    "\n",
    "# Il DataFrame 'df_medie_stagionali' ora contiene le feature che ci servono\n",
    "# e andrà a sostituire il vecchio 'df_rapporto_stagionale' nell'unione finale.\n",
    "print(\"4. Calcolo delle medie per 4 stagioni completato.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e53d3-62a3-4c48-8842-0a127fc95e05",
   "metadata": {},
   "source": [
    "### Feature 5: Eventi Estremi (Rischio Sanitario)\n",
    "\n",
    "**Obiettivo:** Misurare non solo il comportamento medio, ma anche la frequenza degli eventi ad alto inquinamento. Calcoliamo il numero totale di giorni in cui la media giornaliera di PM10 ha superato la soglia di legge di 50 µg/m³. Questa feature è un indicatore diretto del rischio per la salute in quella zona.\n",
    "\n",
    "**Risultato:** Il DataFrame `df_superamenti` conterrà una riga per ogni stazione e una singola colonna, `giorni_superamento_soglia_PM10`, con il conteggio totale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527a4ed-2af3-4e30-8d56-38e181f78265",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5. Calcolo dei Superamenti Soglia per PM10...\")\n",
    "param_pm10 = 'PM10' # Assicurati che questo nome sia corretto\n",
    "soglia_pm10 = 50\n",
    "df_pm10 = df_lavoro[df_lavoro['PARAMETRO'] == param_pm10]\n",
    "\n",
    "# --- INIZIO DELLA CORREZIONE ---\n",
    "# Metodo robusto con .apply() per evitare l'errore di indicizzazione\n",
    "\n",
    "# 1. Definiamo una funzione che opera su un piccolo DataFrame (i dati di una singola stazione)\n",
    "def count_daily_exceedances(group):\n",
    "    # Se il gruppo è vuoto, non ci sono superamenti\n",
    "    if group.empty:\n",
    "        return 0\n",
    "    # Impostiamo la data come indice per poter usare resample\n",
    "    group = group.set_index('DATA_INIZIO')\n",
    "    # Calcoliamo la media giornaliera\n",
    "    daily_mean = group['VALORE'].resample('D').mean()\n",
    "    # Contiamo quante di queste medie giornaliere superano la soglia\n",
    "    return (daily_mean > soglia_pm10).sum()\n",
    "\n",
    "# 2. Applichiamo questa funzione a ogni gruppo di stazioni\n",
    "#anche se lancia warning non ci interessa perchè non tocco la colonna di raggruppamento\n",
    "df_superamenti = df_pm10.groupby('id_stazione').apply(count_daily_exceedances, include_groups=False).to_frame(name='giorni_superamento_soglia_PM10')\n",
    "# --- FINE DELLA CORREZIONE ---\n",
    "\n",
    "print(\"5. Calcolo dei Superamenti Soglia per PM10 completato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55db2c-b47c-4f62-91e4-a1af6033f7b1",
   "metadata": {},
   "source": [
    "### Feature 6: Tipo di Area (Contesto Urbanistico)\n",
    "\n",
    "**Obiettivo:** Classificare ogni stazione in base al contesto urbano in cui si trova. Abbiamo definito una feature categorica che distingue tra stazioni situate in un **Capoluogo di provincia** e quelle in altri comuni (**Provincia**). Questa distinzione ci aiuta a separare i pattern delle grandi aree metropolitane da quelli delle zone meno densamente popolate.\n",
    "\n",
    "Per rendere questa informazione utilizzabile dal modello di machine learning, è stata trasformata in colonne numeriche (es. `Tipo_Area_Capoluogo` con valore 1 o 0) tramite One-Hot Encoding.\n",
    "\n",
    "**Risultato:** Il DataFrame `df_area_tipo` contiene, per ogni stazione, queste nuove colonne binarie che ne descrivono il contesto urbanistico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8629de6-5791-41a6-8a77-c0b0ceb69c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Calcolo Feature 6: Tipo di Area ---\")\n",
    "\n",
    "# 1. Definiamo i capoluoghi di provincia dell'Emilia-Romagna, basandoci sulla lista fornita\n",
    "capoluoghi = [\n",
    "    'BOLOGNA', 'FERRARA', 'FORLI\\'', 'CESENA', 'MODENA', 'PARMA', \n",
    "    'PIACENZA', 'RAVENNA', 'REGGIO NELL\\'EMILIA', 'RIMINI'\n",
    "    # Nota: Cesena è capoluogo insieme a Forlì, ma potrebbe apparire come comune a sé\n",
    "]\n",
    "\n",
    "# 2. Creiamo una tabella pulita con le informazioni anagrafiche, se non già esistente\n",
    "if 'df_stazioni_info' not in locals():\n",
    "    colonne_anagrafiche = ['id_stazione', 'Stazione', 'COMUNE', 'PROVINCIA', 'Coord_X', 'Coord_Y', 'LON_GEO', 'LAT_GEO']\n",
    "    df_stazioni_info = df_stazioni_anagrafica[colonne_anagrafiche].drop_duplicates(subset=['id_stazione'])\n",
    "\n",
    "# 3. Creiamo la feature categorica 'Tipo_Area'\n",
    "# Usiamo .str.upper() per rendere il confronto insensibile a maiuscole/minuscole\n",
    "df_stazioni_info['Tipo_Area'] = df_stazioni_info['COMUNE'].str.upper().apply(\n",
    "    lambda comune: 'Capoluogo' if comune in capoluoghi else 'Provincia'\n",
    ")\n",
    "\n",
    "# 4. Applichiamo One-Hot Encoding\n",
    "df_area_tipo = pd.get_dummies(\n",
    "    df_stazioni_info[['id_stazione', 'Tipo_Area']], \n",
    "    columns=['Tipo_Area'], \n",
    "    prefix='Tipo_Area'\n",
    ").astype(int)\n",
    "\n",
    "print(\"Calcolo 'Tipo di Area' completato.\")\n",
    "print(\"Anteprima del nuovo DataFrame di feature:\")\n",
    "display(df_area_tipo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1764fc-26f4-4317-a3d4-57fb679f1502",
   "metadata": {},
   "source": [
    "### Feature 7: Densità di Stazioni Vicine (Contesto della Rete)\n",
    "\n",
    "**Obiettivo:** Misurare il grado di \"isolamento\" di una stazione all'interno della rete di monitoraggio. Per ogni stazione, abbiamo calcolato quante altre stazioni si trovano entro un **raggio di 20 km**, utilizzando le loro coordinate geografiche (`LON_GEO`, `LAT_GEO`).\n",
    "\n",
    "- Un valore **alto** indica che la stazione fa parte di una rete fitta, tipicamente in un'area di grande interesse o criticità (es. un'area urbana).\n",
    "- Un valore **basso (o zero)** indica una stazione più isolata, probabilmente con funzione di monitoraggio di fondo, rurale o montano.\n",
    "\n",
    "**Risultato:** Il DataFrame `df_densita` contiene, per ogni stazione, la colonna `stazioni_vicine_20km` con il conteggio delle stazioni limitrofe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7eaf0-6870-43ca-af6b-be3c0c17ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Calcolo Feature 7: Densità di Stazioni Vicine ---\")\n",
    "\n",
    "# 1. Usiamo le coordinate geografiche pulite dal nostro DataFrame anagrafico\n",
    "df_coords = df_stazioni_info[['id_stazione', 'LAT_GEO', 'LON_GEO']].dropna()\n",
    "\n",
    "# 2. Convertiamo i gradi in radianti per il calcolo matematico\n",
    "df_coords['lat_rad'] = np.radians(df_coords['LAT_GEO'])\n",
    "df_coords['lon_rad'] = np.radians(df_coords['LON_GEO'])\n",
    "\n",
    "# 3. Calcoliamo la matrice delle distanze (in km) tra ogni coppia di stazioni\n",
    "#    haversine_distances è lo strumento perfetto per questo.\n",
    "dist_matrix_km = haversine_distances(df_coords[['lat_rad', 'lon_rad']]) * 6371  # Raggio medio della Terra in km\n",
    "\n",
    "# 4. Per ogni stazione (riga della matrice), contiamo quante altre sono nel raggio definito\n",
    "raggio_km = 20\n",
    "# La condizione (row > 0) è fondamentale per escludere la distanza di una stazione con se stessa.\n",
    "stazioni_vicine = [np.sum((row > 0) & (row <= raggio_km)) for row in dist_matrix_km]\n",
    "\n",
    "# 5. Creiamo il DataFrame finale con i risultati\n",
    "df_densita = pd.DataFrame({\n",
    "    'id_stazione': df_coords['id_stazione'],\n",
    "    f'stazioni_vicine_{raggio_km}km': stazioni_vicine\n",
    "})\n",
    "\n",
    "print(\"Calcolo 'Densità di Stazioni Vicine' completato.\")\n",
    "print(\"Anteprima del nuovo DataFrame di feature:\")\n",
    "display(df_densita.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbe36f-c199-4203-90d0-db10eac6cf63",
   "metadata": {},
   "source": [
    "### Fase Finale: Unione, Pulizia e Salvataggio del Silver Layer\n",
    "\n",
    "**Obiettivo:** Assemblare tutti i DataFrame di feature calcolati in un'unica, grande tabella `df_silver_profiles`. Questo passaggio finale include:\n",
    "1.  **Concatenazione** di tutte le feature (statistiche, cicli temporali, correlazioni e geografiche).\n",
    "2.  **Gestione dei valori nulli (`NaN`)** che possono emergere durante i calcoli.\n",
    "3.  **Arricchimento** con i metadati anagrafici (nomi delle stazioni, comuni, province).\n",
    "4.  **Pulizia finale** per rimuovere eventuali stazioni per cui non sono state trovate informazioni anagraf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b42ab0-e2ae-447b-ae6b-7095b8d14f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# FASE FINALE: UNIONE COMPLETA, PULIZIA E SALVATAGGIO\n",
    "# ===================================================================\n",
    "print(\"--- Inizio Fase Finale: Unione, Pulizia e Salvataggio ---\")\n",
    "\n",
    "# --- 1. Unione di TUTTE le 8 categorie di feature calcolate ---\n",
    "print(\"\\n[Passaggio 1] Unione di tutti i DataFrame di feature...\")\n",
    "# Mettiamo tutti i DataFrame (inclusi quelli geografici) nella lista\n",
    "df_list = [\n",
    "    df_stats, \n",
    "    df_rapporto_settimanale, \n",
    "    df_medie_stagionali, \n",
    "    df_superamenti, \n",
    "    df_area_tipo.set_index('id_stazione'), \n",
    "    df_densita.set_index('id_stazione')\n",
    "]\n",
    "df_silver_profiles = pd.concat(df_list, axis=1)\n",
    "print(f\"Tabella concatenata creata con {df_silver_profiles.shape} colonne di feature.\")\n",
    "\n",
    "# --- 2. Gestione dei valori NaN (numerici) ---\n",
    "print(\"[Passaggio 2] Gestione dei valori nulli nelle feature numeriche...\")\n",
    "# Riempiamo i valori mancanti come prima\n",
    "df_silver_profiles.fillna(0, inplace=True)\n",
    "for col in df_silver_profiles.columns:\n",
    "    if 'rapporto' in col:\n",
    "        df_silver_profiles[col] = df_silver_profiles[col].replace(0, 1)\n",
    "    if 'corr' in col:\n",
    "        if '_vs_' in col and col.split('_vs_') == col.split('_vs_'):\n",
    "            df_silver_profiles[col] = df_silver_profiles[col].replace(0, 1)\n",
    "        else:\n",
    "            df_silver_profiles[col] = df_silver_profiles[col].replace(0, 0)\n",
    "print(\"Valori nulli numerici gestiti.\")\n",
    "\n",
    "# --- 3. Arricchimento con i metadati anagrafici ---\n",
    "print(\"[Passaggio 3] Unione con i metadati anagrafici...\")\n",
    "# Trasformiamo l'indice 'id_stazione' in una colonna per il merge\n",
    "df_silver_profiles.reset_index(inplace=True)\n",
    "\n",
    "# Eseguiamo il merge con le informazioni delle stazioni\n",
    "df_silver_profiles = pd.merge(df_silver_profiles, df_stazioni_info, on='id_stazione', how='left')\n",
    "print(\"Merge con anagrafica completato.\")\n",
    "\n",
    "# --- 4. Pulizia finale: gestione delle stazioni senza anagrafica ---\n",
    "print(\"[Passaggio 4] Pulizia delle stazioni con anagrafica mancante...\")\n",
    "righe_con_nan_anagrafica = df_silver_profiles['COMUNE'].isnull().sum()\n",
    "if righe_con_nan_anagrafica > 0:\n",
    "    print(f\"Trovate {righe_con_nan_anagrafica} stazioni con metadati mancanti. Verranno rimosse dall'analisi finale.\")\n",
    "    df_silver_profiles.dropna(subset=['COMUNE'], inplace=True)\n",
    "    print(f\"Stazioni rimosse. La tabella finale ora contiene {len(df_silver_profiles)} stazioni.\")\n",
    "    display(df_silver_profiles)\n",
    "else:\n",
    "    print(\"Nessuna stazione con anagrafica mancante. Ottimo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c62fe-a879-43dd-acf6-971d3a63c3fa",
   "metadata": {},
   "source": [
    "### Fase Finale: Test Estensivi di Qualità e Coerenza del Silver Layer\n",
    "\n",
    "**Obiettivo:** Eseguire una serie di controlli approfonditi sulla tabella finale `df_silver_profiles` per validarne la qualità prima di procedere con il machine learning. Questi test vanno oltre la semplice presenza di dati, verificando:\n",
    "- **Completezza:** Assenza di valori nulli non gestiti.\n",
    "- **Integrità Strutturale:** Unicità degli identificativi delle stazioni.\n",
    "- **Coerenza Logica:** I valori calcolati rispettano vincoli logici (es. deviazioni standard non negative).\n",
    "- **Validità di Dominio:** I valori rientrano in range plausibili (es. correlazioni tra -1 e 1).\n",
    "- **Coerenza Geografica:** Le coordinate delle stazioni sono plausibili per la regione di studio.\n",
    "\n",
    "Superare questi test ci dà un'alta fiducia nella qualità del dataset che useremo per il clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe3b21-152e-4fe0-b96e-019092c7faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- INIZIO DEI TEST SULLA TABELLA SILVER FINALE ---\")\n",
    "\n",
    "# Assicurati che df_silver_profiles sia il tuo DataFrame finale e completo\n",
    "# (già unito con tutte le feature, inclusi i metadati)\n",
    "\n",
    "# Test 1: Controllo dei Valori Nulli (Completezza)\n",
    "print(\"\\n[Test 1] Controllo Valori Nulli...\")\n",
    "valori_nulli = df_silver_profiles.isnull().sum().sum()\n",
    "if valori_nulli == 0:\n",
    "    print(\"PASSATO: La tabella non contiene valori nulli (NaN). Il dataset è completo.\")\n",
    "else:\n",
    "    print(f\"FALLITO: Sono stati trovati {valori_nulli} valori nulli. Ispezionare le seguenti colonne:\")\n",
    "    print(df_silver_profiles.isnull().sum()[df_silver_profiles.isnull().sum() > 0])\n",
    "\n",
    "# Test 2: Unicità delle Stazioni (Integrità Strutturale)\n",
    "print(\"\\n[Test 2] Controllo Unicità Stazioni...\")\n",
    "if df_silver_profiles['id_stazione'].is_unique:\n",
    "    print(f\"PASSATO: Ogni riga rappresenta una stazione unica. L'ID è una chiave primaria valida.\")\n",
    "else:\n",
    "    print(f\"FALLITO: Sono presenti ID di stazione duplicati. Questo è un problema critico.\")\n",
    "    duplicati = df_silver_profiles[df_silver_profiles['id_stazione'].duplicated()]['id_stazione']\n",
    "    print(f\"ID duplicati: {duplicati.tolist()}\")\n",
    "\n",
    "# Test 3: Coerenza Logica dei Valori Calcolati\n",
    "print(\"\\n[Test 3] Controllo Coerenza Logica (Valori Non Negativi)...\")\n",
    "# Deviazioni standard, rapporti e conteggi non possono essere negativi.\n",
    "colonne_da_testare = [c for c in df_silver_profiles.columns if c.startswith('std_') or c.startswith('rapporto_') or c.startswith('giorni_')]\n",
    "min_valori = df_silver_profiles[colonne_da_testare].min()\n",
    "if (min_valori >= 0).all():\n",
    "    print(\"PASSATO: Tutte le deviazioni standard, i rapporti e i conteggi sono correttamente maggiori o uguali a zero.\")\n",
    "else:\n",
    "    print(\"FALLITO: Trovati valori negativi dove non dovrebbero esserci. Dettaglio:\")\n",
    "    print(min_valori[min_valori < 0])\n",
    "\n",
    "# Test 4: Validità di Dominio delle Correlazioni\n",
    "print(\"\\n[Test 4] Controllo Range Valori di Correlazione...\")\n",
    "colonne_corr = [col for col in df_silver_profiles.columns if 'corr_' in str(col)]\n",
    "min_corr = df_silver_profiles[colonne_corr].min().min()\n",
    "max_corr = df_silver_profiles[colonne_corr].max().max()\n",
    "if min_corr >= -1 and max_corr <= 1:\n",
    "    print(f\"PASSATO: Tutti i valori di correlazione sono correttamente compresi tra -1 e 1.\")\n",
    "else:\n",
    "    print(f\"FALLITO: Trovati valori di correlazione non validi. Min: {min_corr}, Max: {max_corr}\")\n",
    "\n",
    "# Test 5: Coerenza tra Feature Correlate\n",
    "print(\"\\n[Test 5] Controllo Coerenza tra Media e Deviazione Standard...\")\n",
    "# Se la media di un inquinante è 0, anche la sua deviazione standard deve essere 0.\n",
    "colonne_mean = [c for c in df_silver_profiles.columns if c.startswith('mean_')]\n",
    "incoerenze = 0\n",
    "for col_mean in colonne_mean:\n",
    "    col_std = col_mean.replace('mean_', 'std_')\n",
    "    if col_std in df_silver_profiles.columns:\n",
    "        # Trova le righe dove la media è 0 ma la std non lo è\n",
    "        stazioni_incoerenti = df_silver_profiles[(df_silver_profiles[col_mean] == 0) & (df_silver_profiles[col_std] != 0)]\n",
    "        if not stazioni_incoerenti.empty:\n",
    "            incoerenze += 1\n",
    "            print(f\"  - FALLITO per {col_mean}: {len(stazioni_incoerenti)} stazioni hanno media 0 ma std != 0.\")\n",
    "if incoerenze == 0:\n",
    "    print(\"PASSATO: La deviazione standard è coerente con la media per tutte le feature.\")\n",
    "\n",
    "# Test 6: Coerenza Geografica (Bounding Box)\n",
    "print(\"\\n[Test 6] Controllo Coerenza Geografica delle Coordinate...\")\n",
    "# Definiamo un \"recinto\" geografico approssimativo per l'Emilia-Romagna\n",
    "lat_min, lat_max = 43.7, 45.1\n",
    "lon_min, lon_max = 9.2, 12.7\n",
    "lat_fuori_range = df_silver_profiles[(df_silver_profiles['LAT_GEO'] < lat_min) | (df_silver_profiles['LAT_GEO'] > lat_max)]\n",
    "lon_fuori_range = df_silver_profiles[(df_silver_profiles['LON_GEO'] < lon_min) | (df_silver_profiles['LON_GEO'] > lon_max)]\n",
    "if lat_fuori_range.empty and lon_fuori_range.empty:\n",
    "    print(\"PASSATO: Tutte le coordinate delle stazioni rientrano nei confini geografici attesi per l'Emilia-Romagna.\")\n",
    "else:\n",
    "    print(\"FALLITO: Trovate stazioni con coordinate geografiche non plausibili.\")\n",
    "    if not lat_fuori_range.empty:\n",
    "        print(f\"Stazioni con latitudine fuori range: {lat_fuori_range['id_stazione'].tolist()}\")\n",
    "    if not lon_fuori_range.empty:\n",
    "        print(f\"Stazioni con longitudine fuori range: {lon_fuori_range['id_stazione'].tolist()}\")\n",
    "\n",
    "\n",
    "# Test 7 (Finale): Ispezione Manuale delle Statistiche\n",
    "print(\"\\n[Test 7] Analisi Statistiche Descrittive (Campione) per Ispezione Manuale...\")\n",
    "print(\"Controllare manualmente la colonna 'std' (deviazione standard) per valori molto alti che potrebbero indicare anomalie o outlier.\")\n",
    "colonne_campione = [\n",
    "    'mean_PM10', 'std_PM10', \n",
    "    'rapporto_feriale_festivo_NO2_Biossido_di_azoto',\n",
    "    'media_inverno_PM10', 'media_estate_PM10',\n",
    "    'corr_NO2_Biossido_di_azoto_vs_PM10',\n",
    "    'giorni_superamento_soglia_PM10',\n",
    "    f'stazioni_vicine_20km',\n",
    "    'Tipo_Area_Capoluogo'\n",
    "]\n",
    "colonne_campione_esistenti = [col for col in colonne_campione if col in df_silver_profiles.columns]\n",
    "display(df_silver_profiles[colonne_campione_esistenti].describe().T)\n",
    "\n",
    "print(\"\\n--- TEST COMPLETATI ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b733819-31eb-4582-b68e-47588e095b5a",
   "metadata": {},
   "source": [
    "## FASE FINALE: SALVATAGGIO LOCALE DEL SILVER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b0336-2a39-41bf-9f3a-11d1060067e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# FASE FINALE: SALVATAGGIO LOCALE DEL SILVER LAYER\n",
    "# ===================================================================\n",
    "print(\"--- Inizio Fase di Salvataggio Locale ---\")\n",
    "\n",
    "# 1. Definiamo il percorso locale dove salvare la tabella Silver\n",
    "#    Assicurati che la cartella 'delta_tables_silver' esista allo stesso livello di 'scripts' e 'notebooks'\n",
    "#    Se non esiste, creala con il comando 'mkdir delta_tables_silver' dal terminale.\n",
    "local_silver_path = \"../delta_tables/profili_stazioni_silver\"\n",
    "\n",
    "# 2. Ci assicuriamo che l'indice sia una colonna normale, come richiesto da Delta Lake\n",
    "#    Se df_silver_profiles ha ancora 'id_stazione' come indice, questa riga è necessaria.\n",
    "#    Se è già una colonna, non darà errore.\n",
    "if 'id_stazione' not in df_silver_profiles.columns:\n",
    "    df_silver_profiles.reset_index(inplace=True)\n",
    "\n",
    "# 3. Salviamo il DataFrame in formato Delta Lake nella cartella locale\n",
    "try:\n",
    "    write_deltalake(\n",
    "        local_silver_path,\n",
    "        df_silver_profiles,\n",
    "        mode=\"overwrite\"\n",
    "    )\n",
    "    print(f\"\\nTabella Silver salvata con successo nel percorso locale:\")\n",
    "    print(local_silver_path)\n",
    "    print(\"\\nOra puoi eseguire lo script di upload per caricarla su MinIO.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRORE durante il salvataggio locale: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
